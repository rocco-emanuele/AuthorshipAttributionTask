{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2208a0ca",
   "metadata": {},
   "source": [
    "# Functions and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6524442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.optimize import curve_fit\n",
    "from math import log\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1be25b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads the corpus in a dictionary corpus[author][book][text] \n",
    "#The files have to be stored in a directory organized by: author -> book -> text for the function to work properly\n",
    "def load_corpus_by_author_tokenized(\n",
    "    root_dir,\n",
    "    merge_fragments=True,\n",
    "    token_length=4,\n",
    "    max_tokens=None\n",
    "):\n",
    "\n",
    "    pattern = re.compile(r\"^(.*?)(\\d+)([a-z]?)\\.txt$\", re.IGNORECASE)\n",
    "\n",
    "    corpus = defaultdict(lambda: defaultdict(dict))\n",
    "    punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for author in os.listdir(root_dir):\n",
    "        author_path = os.path.join(root_dir, author)\n",
    "        if not os.path.isdir(author_path):\n",
    "            continue\n",
    "\n",
    "        for book in os.listdir(author_path):\n",
    "            book_path = os.path.join(author_path, book)\n",
    "            if not os.path.isdir(book_path):\n",
    "                continue\n",
    "\n",
    "\n",
    "            grouped = defaultdict(list)\n",
    "            for fname in os.listdir(book_path):\n",
    "                if not fname.lower().endswith(\".txt\"):\n",
    "                    continue\n",
    "                m = pattern.match(fname)\n",
    "                if not m:\n",
    "                    continue\n",
    "                prefix, number, suffix = m.groups()\n",
    "                file_id = f\"{prefix}{number}\" if merge_fragments else f\"{prefix}{number}{suffix}\"\n",
    "                grouped[file_id].append(fname)\n",
    "\n",
    "\n",
    "            for file_id, fragment_list in grouped.items():\n",
    "                fragment_list.sort()  \n",
    "                text = \"\"\n",
    "                for fname in fragment_list:\n",
    "                    path = os.path.join(book_path, fname)\n",
    "                    try:\n",
    "                        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                            text += f.read() + \"\\n\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {path}: {e}\")\n",
    "\n",
    "\n",
    "                if token_length == 0:\n",
    "                    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "                elif token_length != 0:\n",
    "                    tokens = []\n",
    "                    text = text.lower()\n",
    "                    s = text.lower().replace('\\n', ' ').replace('\\t', ' ').strip()\n",
    "                    N = token_length\n",
    "                    for i in range(len(s) - N + 1):\n",
    "                        tok = s[i:i+N]\n",
    "\n",
    "                        if tok[1:-1].count(\" \") == 0:\n",
    "                            tokens.append(tok)\n",
    "\n",
    "\n",
    "                if isinstance(max_tokens, int) and max_tokens > 0:\n",
    "                    for i in range(0, len(tokens), max_tokens):\n",
    "                        chunk = tokens[i:i + max_tokens]\n",
    "                        chunk_id = f\"{file_id}_chunk{i//max_tokens+1}\"\n",
    "                        corpus[author][book][chunk_id] = chunk\n",
    "                else:\n",
    "                    corpus[author][book][file_id] = tokens\n",
    "\n",
    "                \n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fcdd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset in training and test \n",
    "def split_corpus_by_books(corpus_by_author, train_books_by_author):\n",
    "\n",
    "    train_set = defaultdict(lambda: defaultdict(dict))\n",
    "    test_set = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "    for author, books in corpus_by_author.items():\n",
    "        train_books = train_books_by_author.get(author, [])\n",
    "        for book, files in books.items():\n",
    "            target_set = train_set if book in train_books else test_set\n",
    "            for fname, text in files.items():\n",
    "                target_set[author][book][fname] = text\n",
    "\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10addf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms the structure of the dataset dictionary from corpus[author][book][text] to corpus[author][texts]\n",
    "#this operation is necessary to compute the author parameters\n",
    "def flatten_corpus(corpus_nested):\n",
    "\n",
    "    return {\n",
    "        author: [\n",
    "            tokens\n",
    "            for books in corpus_nested[author].values()\n",
    "            for tokens in books.values()\n",
    "        ]\n",
    "        for author in corpus_nested\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca56a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions used to compute the heaps' law \n",
    "def heaps_curve_texts(texts):\n",
    "\n",
    "    seen = set() \n",
    "    t = 0\n",
    "    out = []\n",
    "    for txt in texts:\n",
    "        for w in txt:\n",
    "            t += 1\n",
    "            seen.add(w)\n",
    "            out.append((t, len(seen)))\n",
    "    return np.array(out)\n",
    "\n",
    "def heaps_func(t, alpha, theta):\n",
    "    return (theta/alpha) * ((1 + t/theta)**alpha - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "586f2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a dictionary with all the information needed to do the attribution, divided by author\n",
    "def build_author_model(texts, alpha, theta, P0, delta=1.0):\n",
    "    tokens = []\n",
    "    for txt in texts:\n",
    "        tokens += txt\n",
    "    \n",
    "    m = len(tokens)\n",
    "    count_A = Counter(tokens)\n",
    "    D_A = len(count_A)\n",
    "    if METHODP==\"global\":\n",
    "        P0_A = {w: delta*P0[w] for w in P0}\n",
    "    else:\n",
    "        unseen = {w: P0[w] for w in P0 if w not in count_A}\n",
    "        Z = sum(unseen.values())\n",
    "        P0_A = {w: delta*unseen[w]/Z for w in unseen}\n",
    "    return {\n",
    "        \"count_A\": count_A,\n",
    "        \"m\": m,\n",
    "        \"D_A\": D_A,\n",
    "        \"alpha\": alpha,\n",
    "        \"theta\": theta,\n",
    "        \"P0_A\": P0_A\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea877ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to compute the conditional probability formula as written in the paper\n",
    "from scipy.special import loggamma, poch\n",
    "\n",
    "def log_pochhammer(z, n, step=1):\n",
    "    if n <= 0:\n",
    "        return 0.0\n",
    "    terms = z + np.arange(n) * step\n",
    "    return np.log(terms).sum()\n",
    "\n",
    "def log_cond_prob(tokens_T, model):\n",
    "    count_T = Counter(tokens_T)\n",
    "    n=sum(count_T.values())\n",
    "    \n",
    "    m = model[\"m\"]\n",
    "    D_A = model[\"D_A\"]\n",
    "    alpha = model[\"alpha\"]\n",
    "    theta = model[\"theta\"]\n",
    "    count_A = model[\"count_A\"]\n",
    "    P0_A = model[\"P0_A\"]\n",
    "    S_Qj = 0\n",
    "    \n",
    "\n",
    "    D_union = len(set(count_A.keys()) | set(count_T.keys()))\n",
    "\n",
    "    K = D_union - D_A\n",
    "    logB = log_pochhammer(theta + alpha*D_A, K, alpha) - log_pochhammer(theta + m, n, 1)\n",
    "\n",
    "    \n",
    "\n",
    "    for w, nj in count_T.items():\n",
    "        if w in count_A:\n",
    "            log_Qj = log_pochhammer(count_A[w] - alpha, nj, 1)\n",
    "        else:\n",
    "            p0 = P0_A.get(w, 1e-10)\n",
    "            if p0!=0:\n",
    "                log_Qj = log_pochhammer(1-alpha, nj-1, 1) + np.log(p0)\n",
    "            elif p0==0:\n",
    "                log_Qj = log_pochhammer(1-alpha, nj-1, 1)\n",
    "                \n",
    "        S_Qj += log_Qj \n",
    "    return logB + S_Qj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d13920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attribution method function. Majority or full text\n",
    "#if METHOD=MR assigns every fragment in a text indipendently and chooses the author with more fragments assigned\n",
    "#otherwise the full text is assigned (the probability is the sum of the fragments probabilities)\n",
    "#in both cases the function log_cond_prob is used to compute the probability\n",
    "def assign_corpus_by_chunks(corpus_by_author,models,aggregation=\"majority\"):\n",
    "    assigned=defaultdict(lambda: defaultdict(dict))\n",
    "    for true_a,books in corpus_by_author.items():\n",
    "        for book,files in books.items():\n",
    "            grouping=defaultdict(list)\n",
    "            for fid,toks in files.items():\n",
    "                base=fid.split(\"_chunk\")[0]\n",
    "                grouping[base].append(toks)\n",
    "            for base,chunks in grouping.items():\n",
    "                if aggregation==\"majority\":\n",
    "                    votes=Counter()\n",
    "                    for ch in chunks:\n",
    "                        scores={a:log_cond_prob(ch,mdl) for a,mdl in models.items()}\n",
    "                        votes[max(scores,key=scores.get)]+=1\n",
    "                    best=votes.most_common(1)[0][0]\n",
    "                else:  # logsum\n",
    "                    totals={a:0.0 for a in models}\n",
    "                    for ch in chunks:\n",
    "                        for a,mdl in models.items():\n",
    "                            totals[a]+=log_cond_prob(ch,mdl)\n",
    "                    best=max(totals,key=totals.get)\n",
    "                assigned[true_a][book][base]=best\n",
    "    return assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e78cf968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves the result of an iteration in a file txt with: token_length, books used in the Training_set, P0 author or global, results\n",
    "# and for every wrong attribution the correct result along with the wrong prediction \n",
    "from datetime import datetime\n",
    "\n",
    "def save_results_report(\n",
    "    results,                  \n",
    "    train_books_by_author,    \n",
    "    token_length,             \n",
    "    P0_type= \"global\",         \n",
    "    output_path=\"risultati.txt\"\n",
    "):\n",
    "    total = len(results)\n",
    "    correct = sum(1 for t in results if t[0] == t[3])\n",
    "    incorrect = total - correct\n",
    "    acc = 100 * correct / total if total > 0 else 0\n",
    "\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"=== REPORT ===\\n\")\n",
    "        f.write(f\"Date and time: {now}\\n\\n\")\n",
    "        f.write(f\" Token length: {token_length}\\n\")\n",
    "        f.write(f\" P0 : {P0_type}\\n\")\n",
    "        f.write(f\" Training_set:\\n\")\n",
    "        for author, books in train_books_by_author.items():\n",
    "            f.write(f\"    - {author}: {', '.join(books)}\\n\")\n",
    "\n",
    "        f.write(f\"\\n Results:\\n\")\n",
    "        f.write(f\"    - Total number of texts to be assigned: {total}\\n\")\n",
    "        f.write(f\"    - Correct:     {correct}\\n\")\n",
    "        f.write(f\"    - Wrong:       {incorrect}\\n\")\n",
    "        f.write(f\"    - Accuracy:     {acc:.2f}%\\n\")\n",
    "\n",
    "        if incorrect > 0:\n",
    "            f.write(f\"\\n Texts assigned improperly:\\n\")\n",
    "            for true_a, book, base, pred in results:\n",
    "                if true_a != pred:\n",
    "                    f.write(f\"    - {base} (book: {book}) → predicted: {pred}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4710b3cc",
   "metadata": {},
   "source": [
    "# Attribution Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a34e5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit bounds and points \n",
    "N_SAMPLE = 50000\n",
    "EPS = 1e-5\n",
    "#Bounds: 0<alpha<1, theta>0\n",
    "BOUNDS = ([EPS, EPS], [1-EPS, 1e6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d4b1d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus directory and attribution's parameters\n",
    "CORPUS_DIR = \"Corpus\"\n",
    "\n",
    "TOK_LENG = 2\n",
    "FRAG_N = None \n",
    "DELTA = 1\n",
    "\n",
    "#METHOD set the rule used by the algorithm (MR= majority rule, FNN= full text)(MR works only if the FRAG_N isn't None)\n",
    "METHOD = \"FNN\" #\"MR\" \n",
    "#METHODP set the normalization of P0 (author= normalized by author removing 'words' which appear in the author corpus, global= frequency of token in the entire corpus)\n",
    "METHODP = \"global\" # \"author\"\n",
    "#MERGE=True merges the texts which have a suffix letter (ex: text1a and text1b becomes one single text named text1)\n",
    "MERGE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b38b3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving variables (file_name and directory to be saved it in)\n",
    "PATH = \"results\"\n",
    "PATHDIR = \"EsperimentiLibri\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2c3165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choice of trainingset books\n",
    "train_books_by_author = {\n",
    "    \"Ovidio Amores\": [\"Ovidio Amores I\"],\n",
    "    \"Properzio\": [\"Properzio I\"],\n",
    "    \"Tibullo\": [\"Tibullo I\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6481af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Main \n",
    "corpus_by_author = load_corpus_by_author_tokenized(\n",
    "    root_dir=\"Corpus\",\n",
    "    merge_fragments= MERGE,\n",
    "    token_length= TOK_LENG,\n",
    "    max_tokens= FRAG_N\n",
    ")\n",
    "\n",
    "#Both sets are dictionaries with structure: train_set[author][book][text]\n",
    "train_set, test_set = split_corpus_by_books(corpus_by_author, train_books_by_author)\n",
    "train_flat = flatten_corpus(train_set)\n",
    "\n",
    "\n",
    "#Fit and extrapolation of authors' parameters\n",
    "author_params = {}\n",
    "for auth, texts in train_flat.items():\n",
    "    data = heaps_curve_texts(texts)\n",
    "    t_vals, D_vals = data[:,0], data[:,1]\n",
    "    if len(t_vals) > N_SAMPLE:\n",
    "        idx = np.linspace(0, len(t_vals)-1, N_SAMPLE, dtype=int)\n",
    "        t_fit, D_fit = t_vals[idx], D_vals[idx]\n",
    "    else:\n",
    "        t_fit, D_fit = t_vals, D_vals\n",
    "    p0 = [0.3, len(D_vals)]\n",
    "    try:\n",
    "        popt, _ = curve_fit(\n",
    "            heaps_func, t_fit, D_fit,\n",
    "            p0=p0, bounds=BOUNDS, maxfev=5000\n",
    "        )\n",
    "    except RuntimeError:\n",
    "        popt = p0 \n",
    "        print('FIT ERROR')\n",
    "    author_params[auth] = tuple(popt)\n",
    "    #print(f\"Author {auth}: α = {popt[0]:}, θ = {popt[1]:}\")\n",
    "\n",
    "#computation of the global frequencies of tokens (later they will be multiplied by DELTA and normalized by author if METHODP=author )\n",
    "corpus_flat = flatten_corpus(corpus_by_author)\n",
    "all_tokens = [w for texts in corpus_flat.values() for tok_list in texts for w in tok_list]\n",
    "global_counts = Counter(all_tokens)\n",
    "total = sum(global_counts.values())\n",
    "P0_global = {w: c/total for w,c in global_counts.items()}\n",
    "\n",
    "\n",
    "models = {auth: build_author_model(train_flat[auth], *author_params[auth], P0_global, DELTA) for auth in train_flat}\n",
    "\n",
    "#attribution\n",
    "if METHOD==\"MR\":\n",
    "    assigned_majority = assign_corpus_by_chunks(test_set, models, aggregation=\"majority\")\n",
    "else:\n",
    "    assigned_logsum  = assign_corpus_by_chunks(test_set, models, aggregation=\"logsum\")\n",
    "\n",
    "#results compilation\n",
    "correct=0\n",
    "wrong=0\n",
    "results = []\n",
    "for true_a,books in test_set.items():\n",
    "    for book,files in books.items():\n",
    "        base_files = set(fid.split(\"_chunk\")[0] for fid in files)\n",
    "        for base in base_files:\n",
    "            if METHOD==\"FNN\":\n",
    "                pred = assigned_logsum[true_a][book][base]\n",
    "            elif METHOD==\"MR\":\n",
    "                pred = assigned_majority[true_a][book][base]\n",
    "            results.append((true_a, book, base, pred))\n",
    "\n",
    "            \n",
    "for true_a,book,base,pred in results: \n",
    "    if pred!=true_a:\n",
    "         wrong+=1\n",
    "    if pred==true_a:\n",
    "        correct+=1\n",
    "\n",
    "#Save results in a file.txt      \n",
    "save_results_report(\n",
    "results,\n",
    "train_books_by_author=train_books_by_author,\n",
    "token_length= TOK_LENG,\n",
    "P0_type= METHODP, \n",
    "output_path= PATH + \".txt\")\n",
    "#!move {PATH + \".txt\"} {PATHDIR}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
